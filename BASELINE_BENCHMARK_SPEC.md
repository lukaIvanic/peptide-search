# Baseline Benchmark Spec (v1)

## Overview
This document specifies the first implementation of a baseline benchmark feature for the Peptide Literature Extractor. The goal is to ship a static baseline dataset inside the app, run extractions against the baseline papers, and provide a visual side‑by‑side comparison (baseline vs extracted). Automated scoring is explicitly out of scope for v1. All testing in this phase uses the mock provider only.

## Goals
- Ship baseline datasets inside the repo in a normalized JSON format.
- Provide a `/baseline` UI that shows baseline entries and extraction results side‑by‑side.
- Allow batch extraction over the entire baseline dataset.
- Auto‑match baseline cases to extracted entities by sequence/DOI for visual comparison.

## Non‑goals (v1)
- No automated numeric scoring or ranking.
- No advanced filtering beyond dataset + simple search.
- No manual editing of baseline data inside the app.

## Baseline datasets (current inventory)
All datasets provided in `Peptide LLM/Datasets` are included.

| Dataset ID | Source file | Rows | Notes |
| --- | --- | --- | --- |
| `self_assembly` | `42256_2024_928_MOESM3_ESM.xlsx` | 368 (249 pos + 119 neg) | Positive/negative self‑assembly, includes DOI + validation method |
| `llps` | `LLPS peptides.xlsx` | 76 | LLPS conditions + article link |
| `catalytic_prot` | `Peptides cat. func. prot. AA.csv` | 110 | Kcat/KM, substrate, structure, DOI |
| `catalytic_non_prot` | `Peptides cat. func. non-prot. AA.csv` | 16 | Non‑proteinogenic residues |
| `avp` | `AVP dataset.csv` | 1056 | Antiviral peptides with PubMed/Patent ID |

Total: 1626 baseline cases.

## Baseline data format
Normalized JSON is stored in `app/baseline/data/` with one file per dataset and an index file.

### Case schema (v1)
```
{
  "id": "self_assembly:pos:1",
  "dataset": "self_assembly",
  "sequence": "QEIARLEQEIARLEYEIARLE",
  "n_terminal": "Acetylated",
  "c_terminal": "Amidated",
  "labels": ["self-assembly"],
  "doi": "10.1002/anie.200604014",
  "pubmed_id": null,
  "paper_url": null,
  "pdf_url": null,
  "metadata": {
    "validation_methods_raw": "CD spectroscopy, TEM",
    "source_sheet": "Self-assembling sequences"
  }
}
```

### JSON assets
- `app/baseline/data/index.json` — dataset metadata + counts
- `app/baseline/data/self_assembly.json`
- `app/baseline/data/llps.json`
- `app/baseline/data/catalytic_prot.json`
- `app/baseline/data/catalytic_non_prot.json`
- `app/baseline/data/avp.json`

### Build script
Baseline JSON is generated by:
- `scripts/build_baseline_data.py`
- Requires `openpyxl` (dev‑only); listed in `requirements-dev.txt`

## Baseline rebuild procedure (source-of-truth re-extract)
The baseline is rebuilt by re-fetching papers via DOI/URL and re-running extraction.

1. Start the server with the mock provider configured.
2. Call `POST /api/baseline/enqueue` with:
   - `provider=mock` for coverage checks.
3. Track progress via SSE `/api/stream` (event `run_status`).
4. Verify cases with:
   - `GET /api/baseline/cases`
   - `GET /api/baseline/cases/{case_id}/latest-run`

Operational guidance:
- Use `dataset=catalytic_non_prot` for a minimal mock run.
- Expect missing PDF cases; these should resolve to `failed` with a clear `failure_reason`.

## Shadow mock dataset (comparison testing)
To test comparison behavior without relying on providers, we generate a full-size shadow dataset derived from baseline JSON.

Artifacts:
- `app/baseline/data_shadow/index.json`
- `app/baseline/data_shadow/shadow_extractions.json`

Generation:
- `scripts/build_baseline_shadow_data.py` produces deterministic deltas using a fixed seed.
- Mutations cover: sequence edits, label swaps, missing termini, empty entities, extra entities.

Seeding into DB:
- `scripts/seed_baseline_shadow_runs.py` (dev-only) inserts stored runs for each case id.
- Runs are tagged with `baseline_case_id` / `baseline_dataset` so the `/baseline` UI can compare against the original baseline.
- `POST /api/baseline/shadow-seed` offers an internal API alternative (development only).

## Backend changes

### Database
Add baseline linkage fields to `ExtractionRun`:
- `baseline_case_id: Optional[str]`
- `baseline_dataset: Optional[str]`

Purpose: map extraction runs to static baseline cases without introducing a new DB table.

### Baseline endpoints
New API endpoints:
- `GET /api/baseline/cases`
  - Returns list of baseline cases with latest run summary when available.
  - Optional filter by `dataset`.
- `GET /api/baseline/cases/{case_id}`
  - Returns baseline case detail + latest run summary.
- `GET /api/baseline/cases/{case_id}/latest-run`
  - Returns latest run details for that case, or 404 if none.
- `POST /api/baseline/enqueue`
  - Enqueues batch extraction for baseline cases.
  - Parameters: `provider`, `prompt_id`, optional `dataset` filter, optional `force` (re‑run even if stored).

### DOI/PMID/PDF resolution
Resolution order per baseline case:
1. `pdf_url` from baseline data (if provided).
2. `paper_url` if it looks like a PDF URL.
3. DOI search via `search_all_free_sources(doi)`.
4. PubMed/Patent ID search via `search_all_free_sources(id)`.

If no PDF is found:
- Create a failed `ExtractionRun` linked to the baseline case with a clear `failure_reason`.

### SSE updates
SSE `run_status` events include `baseline_case_id` when available so the UI can update rows without polling.

## Frontend changes

### New page
Add `/baseline` page:
- `public/baseline.html`
- `public/baseline.js`

### UI layout
1. **Top controls**
   - Dataset filter
   - Search box (sequence/DOI/ID)
   - “Run all baseline” button
2. **Baseline case list**
   - Table with dataset, sequence, DOI/ID, latest status
3. **Side‑by‑side comparison**
   - Left: baseline case details (sequence, labels, metadata)
   - Right: extracted run details (entities + raw JSON summary)

### Matching behavior (v1)
Auto‑match extracted entities by sequence:
1. Exact match
2. Case‑insensitive match fallback

If matched:
- Highlight matched entity in the extraction column.
- Display simple match indicators (sequence match, DOI match, label overlap).

No numeric score or ranking is computed in v1.

## Error handling
- If extraction fails, right panel shows failure reason.
- If no run exists for a baseline case, show a “Not run yet” placeholder.

## Testing

### Manual API checklist (internal API)
Baseline:
1. `GET /api/baseline/cases` → returns `cases[]`, `datasets[]`, `total_cases`.
2. `POST /api/baseline/enqueue` with `{"provider":"mock","dataset":"catalytic_non_prot"}` → returns enqueued counts.
3. SSE `/api/stream` → verify `run_status` events include `baseline_case_id`.
4. `GET /api/baseline/cases/{case_id}` → latest run summary present.
5. `GET /api/baseline/cases/{case_id}/latest-run` → run payload includes `raw_json` and `status`.
6. `GET /api/runs/{run_id}` → baseline fields present in run payload.
7. `POST /api/baseline/shadow-seed` (dev) → creates shadow runs for comparison testing.

Full API surface (mock):
8. `GET /api/health` → provider info.
9. `GET /api/search?q=...` → returns results list (may be empty).
10. `POST /api/enqueue` → enqueue a single PDF URL.
11. `GET /api/papers` → list papers with status.
12. `GET /api/papers/{paper_id}/extractions` → extraction list.
13. `GET /api/runs?paper_id=...` → runs list.
14. `GET /api/runs/{run_id}/history` → history list.
15. `POST /api/runs/{run_id}/retry` → requeue failed run.
16. `POST /api/runs/{run_id}/followup` with `provider=mock` → new run created.
17. `POST /api/runs/{run_id}/edit` → new version created.
18. `GET /api/entities` → entities list.
19. `GET /api/entities/{id}` → entity detail.
20. `GET /api/entities/kpis` → KPI summary.
21. `GET /api/quality-rules` → rules payload.
22. `POST /api/quality-rules` → update rules.
23. `GET /api/runs/failure-summary` → summary buckets.
24. `GET /api/runs/failures` → failure list.
25. `POST /api/runs/failures/retry` → bulk retry.
26. `GET /api/prompts` → prompt list.
27. `POST /api/prompts` → create a prompt.
28. `POST /api/prompts/{id}/versions` → create a prompt version.

### Scripted smoke tests
Baseline flow:
- `python scripts/baseline_smoke_test.py --provider mock --dataset catalytic_non_prot --seed-shadow`

Full API surface:
- `python scripts/full_api_smoke_test.py --provider mock --dataset catalytic_non_prot`

The script should:
- enqueue cases,
- poll for completion,
- validate `raw_json` and `entities` shape,
- output a summary report.

### UI validation checklist
1. Open `/baseline` → list renders with dataset counts.
2. Click “Run all baseline” → SSE updates row statuses.
3. Select a case → baseline vs extraction panels render side-by-side.
4. Matched sequences highlight in extraction panel.
5. Failed cases show `failure_reason` on the extraction side.

## Known limits
- DOI/URL resolution may fail due to missing PDFs or restricted access.
- Mock provider does not validate extraction quality; it only validates API flow.
- SSE is best-effort; dropped events can be recovered via `/api/baseline/cases`.

## Future improvements
- Add automated scoring metrics (precision/recall, field coverage).
- Add partial batch selection and retry controls.
- Add export of comparison results.
